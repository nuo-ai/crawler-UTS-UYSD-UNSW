# 技术背景

## 1. 核心技术

- **编程语言**: Python 3.x
- **网页抓取**:
    - `requests`: 用于发出HTTP请求以获取网页内容。
    - `BeautifulSoup4`: 用于解析HTML和XML文档以提取数据。
- **数据处理**:
    - `pandas`: 用于数据操作和结构化。
    - `openpyxl`: 用于将数据写入 `.xlsx` 文件。
- **配置**: `PyYAML`: 用于从 `.yaml` 文件中读取配置。

## 2. 开发环境

- **操作系统**: 脚本在Windows上开发和测试，但已尽可能编写为跨平台兼容。
- **依赖管理**: 所有必需的Python包都列在 `requirements.txt` 中。建议使用虚拟环境进行依赖管理。

## 3. 技术限制与考量

- **网站结构依赖性**: 爬虫的成功高度依赖于目标网站的HTML结构。对网站布局的任何更改都可能破坏爬虫并需要更新代码。
- **速率限制与封锁**: 爬虫必须遵守目标网站的 `robots.txt` 和服务条款。过度的抓取可能导致IP被封锁。当前的实现包括可配置的延迟 (`request_delay`) 以降低此风险。
- **动态内容**: 当前使用 `requests` 和 `BeautifulSoup` 的实现适用于静态网站。它无法处理严重依赖JavaScript来呈现内容的网站。未来的迭代可能需要像Selenium或Playwright这样的工具来处理这种情况。